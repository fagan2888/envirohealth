{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEER Data Analysis\n",
    "# Phase 2: Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MasterSeer class is the primary \"daddy\" class for the entire project. Scripts in all other project phases include inheritance, i.e. \"children\", of this master class. The MasterSeer class manages connections into the sqlite3 database.  A cool feature is the \"if path\" condition which makes this code run on both Macs and PCc-- very useful when team members use different systems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MasterSeer(object):\n",
    "\n",
    "    # database file name on disk\n",
    "    DB_NAME = 'seer.db'\n",
    "\n",
    "    def __init__(self, path = r'./data/', reload = True, verbose = True):\n",
    "\n",
    "        if type(path) != str:\n",
    "            raise TypeError('path must be a string')\n",
    "\n",
    "        if path[-1] != '/':\n",
    "            path += '/'            # if path does not end with a backslash, add one\n",
    "\n",
    "        self.path = path\n",
    "\n",
    "        # List to hold lists of [Column Offset, Column Name, Column Length]\n",
    "        self.dataDictInfo = []\n",
    "        self.db_conn = None\n",
    "        self.db_cur = None\n",
    "\n",
    "    def __del__(self):\n",
    "        self.db_conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The initi_databse function creates a database connection and cursor to sqlite3. If the reload parameter is true, the function deletes all the data from and creates a new empty database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def init_database(self, reload):\n",
    "        try:\n",
    "            if reload:\n",
    "                os.remove(self.path + self.DB_NAME)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            #initialize database\n",
    "            self.db_conn = sqlite3.connect(self.path + self.DB_NAME)\n",
    "            self.db_cur = self.db_conn.cursor()\n",
    "\n",
    "            if self.verbose:\n",
    "                print('Database initialized')\n",
    "\n",
    "            return self.db_conn, self.db_cur\n",
    "\n",
    "        except Exception as e:\n",
    "            print('ERROR connecting to the database: ')\n",
    "            return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next function loads the data dictionary describing the raw SEEER data and returns a dataframe of the data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def load_data_dictionary(self, fname = r'SeerDataDict.txt'):\n",
    "            \n",
    "        REGEX_DD = '\\$char([0-9]+).'\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "        if self.verbose:\n",
    "            print('\\nStart Load of Data Dictionary')\n",
    "\n",
    "        # read our custom tab delimited data dictionary\n",
    "        df = pd.read_csv(self.path + fname, delimiter='\\t')\n",
    "\n",
    "        # drop all rows where IMPORT_0_1 is a zero. \n",
    "        df = df[df.IMPORT_0_1 > 0]\n",
    "\n",
    "        # pre-compile regex to improve performance in loop\n",
    "        reCompiled = re.compile(REGEX_DD)\n",
    "        flen = []       # list to hold parsed field lengths\n",
    "\n",
    "        # add length column\n",
    "        for row in df.TYPE: \n",
    "            fields = reCompiled.match(row)\n",
    "            if fields:\n",
    "                x = int(fields.groups()[0])\n",
    "                flen.append(x)\n",
    "\n",
    "        # check to make sure we read the correct amount of field lengths\n",
    "        if len(flen) != len(df):\n",
    "            print('ERROR reading field lengths')\n",
    "            return None\n",
    "\n",
    "        # add length column to dataframe\n",
    "        df['LENGTH'] = flen\n",
    "\n",
    "        if self.verbose:\n",
    "            print('Data Dictionary loaded in {0:5.4f} sec.'.format(time.perf_counter() - t0), \n",
    "                  flush=True)\n",
    "\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we load the breast cancer data from the sqlite SEER database into a dataframe. We include the column names and determine a sample size. If \"all\" is set to true, the entire table is returned and the sample size is ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def load_data(self, source='breast', col=[], cond=\"YR_BRTH > 0\", sample_size=5000, \n",
    "                  all=False):\n",
    "        \n",
    "        if col:\n",
    "            col = ','.join(map(str, col)) \n",
    "        else:\n",
    "            col = \"*\"\n",
    "\n",
    "        if all:\n",
    "            limit = \"\"\n",
    "            randomize = \"\"\n",
    "        else:\n",
    "            limit = \"LIMIT \" + str(sample_size)\n",
    "            randomize = \"ORDER BY RANDOM()\"\n",
    "\n",
    "        df = pd.read_sql_query\n",
    "        (\"SELECT {0} FROM {1} WHERE {2} {3} {4}\".format(col, source, cond, \n",
    "                                                        randomize, limit), self.db_conn)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next comes the cleaning and recoding function. Each cleaning step is its own row so we can clean only the variables selected through the feature analysis. The cleaning steps remove missing or invalid data, for example \"9\" values for \"unknown\".\n",
    "### This function also recodes a new dependent variable called survival bucket. This turns the continuous survival time varaible into a catagorical variable to allow classification analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    def clean_recode_data(self, df, dependent_cutoffs):\n",
    "        \n",
    "        # drop all rows that have invalid or missing data\n",
    "        try: \n",
    "            df = df.dropna(subset = ['YR_BRTH']) # add column names here as needed\n",
    "        except Exception as err:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            df.LATERAL = df.LATERAL.replace([0, 1,2,3], 1)  # one site = 1\n",
    "            df.LATERAL = df.LATERAL.replace([4,5,9], 2)     # paired = 2\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            df = df[df.O_DTH_CLASS == 0]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            # 0-benign, 1-borderline, 2-in situ, 3-malignant\n",
    "            df = df[df.BEHANAL != 5]\n",
    "            df.BEHANAL = df.BEHANAL.replace([3,4,6], 3)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try: \n",
    "            df = df[df.HST_STGA != 8]\n",
    "            df = df[df.HST_STGA != 9]\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "        try: \n",
    "            # 0-negative, 1-borderline,, 2-positive\n",
    "            df = df[df.ERSTATUS != 4]\n",
    "            df = df[df.ERSTATUS != 9]\n",
    "            df.ERSTATUS = df.ERSTATUS.replace(2, 0)\n",
    "            df.ERSTATUS = df.ERSTATUS.replace(1, 2)\n",
    "            df.ERSTATUS = df.ERSTATUS.replace(3, 1)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try: \n",
    "            # 0-negative, 1-borderline,, 2-positive\n",
    "            df = df[df.PRSTATUS != 4]\n",
    "            df = df[df.PRSTATUS != 9]\n",
    "            df.PRSTATUS = df.PRSTATUS.replace(2, 0)\n",
    "            df.PRSTATUS = df.PRSTATUS.replace(1, 2)\n",
    "            df.PRSTATUS = df.PRSTATUS.replace(3, 1)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            df.RADIATN = df.RADIATN.replace(7, 0)\n",
    "            df.RADIATN = df.RADIATN.replace([2,3,4,5], 1)\n",
    "            df = df[df.RADIATN < 7] \n",
    "        except Exception as err:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            # code as 1 or 2-more than one\n",
    "            df.NUMPRIMS = df.NUMPRIMS.replace([x for x in range(2,37)], 2)\n",
    "        except Exception as err:\n",
    "            pass\n",
    "        \n",
    "        #   Example dependent_cutoffs=[60,120,500]\n",
    "        #   if survival is less than 60 SRV_BUCKET is set to 0\n",
    "        #   if survival is >=60 and < 120 SRV_BUCKET is set to 1\n",
    "        \n",
    "        if len(dependent_cutoffs) > 0:\n",
    "            # create new column of all NaN\n",
    "            df['SRV_BUCKET'] = np.NaN\n",
    "            # fill buckets\n",
    "            last_cut = 0       \n",
    "            for x, cut in enumerate(dependent_cutoffs):\n",
    "                df.loc[(df.SRV_TIME_MON >= last_cut) & \n",
    "                       (df.SRV_TIME_MON < cut), 'SRV_BUCKET'] = x\n",
    "                last_cut = cut\n",
    "            # assign all values larger than last cutoff to next bucket number       \n",
    "            df['SRV_BUCKET'].fillna(len(dependent_cutoffs), inplace=True)\n",
    "\n",
    "            dep_col = 'SRV_BUCKET'\n",
    "            df = df.drop('SRV_TIME_MON', 1)\n",
    "        else:\n",
    "            dep_col = 'SRV_TIME_MON'\n",
    "\n",
    "        # categorical columns to one hot encode, check to make sure they are in df\n",
    "        #cat_cols_to_encode = list(set(['RACE', 'ORIGIN', 'SEX', 'TUMOR_2V', 'HISTREC']) \n",
    "        # & set(df.columns))\n",
    "        #df = self.one_hot_data(df, cat_cols_to_encode)\n",
    "\n",
    "        df['CENSORED'] = df.STAT_REC == 4\n",
    "        df = df.drop('STAT_REC', 1)\n",
    "\n",
    "\n",
    "        df.replace([np.inf, -np.inf], np.nan)\n",
    "        df = df.fillna(0)\n",
    "\n",
    "        exc = pd.ExcelWriter('clean.xlsx')\n",
    "        df.to_excel(exc)\n",
    "        exc.save()\n",
    "\n",
    "        return df, dep_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last, we use the one-hot-encode procedure to return a new dataframe with vectorized data. We use this function only when we want to call standard distance metrics (such as k-nearest neighbors) for catagorical variables (e.g. race). See this link to learn more: http://stackoverflow.com/questions/17469835/one-hot-encoding-for-machine-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "     def one_hot_data(self, data, cols):\n",
    "       \n",
    "        # check to only encode columns that are in the data\n",
    "        col_to_process = [c for c in cols if c in data]\n",
    "        return pd.get_dummies(data, columns = col_to_process,  prefix = col_to_process)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
